{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9634529,"sourceType":"datasetVersion","datasetId":5882313},{"sourceId":9723716,"sourceType":"datasetVersion","datasetId":5882405},{"sourceId":9725644,"sourceType":"datasetVersion","datasetId":5914761},{"sourceId":201840535,"sourceType":"kernelVersion"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport random\nimport numpy as np\nimport config\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport joblib\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\n\nconfig.training_label = \"/kaggle/input/data-k\"\nconfig.max_length = 15\nconfig.epochs = 50\nconfig.batch_size = 64\nconfig.num_decoder_tokens = 6000\n# config.num_decoder_tokens = 1500\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\nclass VideoDataset():\n    def __init__(self):\n        self.x_data = {}\n        self.encoder = EncoderModel().to(device)  \n        self.decoder = DecoderModel().to(device)  \n        self.lr = config.learning_rate\n        self.epochs = config.epochs\n        self.save_model_path = config.save_model_path\n        self.patience = 3  \n        self.min_delta = 0.001\n    def preprocessing(self):\n  \n        TRAIN_LABEL_PATH = os.path.join(config.training_label, '/kaggle/input/data-k/data (2).json')\n        with open(TRAIN_LABEL_PATH) as data_file:\n            y_data = json.load(data_file)\n        \n        train_list = []\n        vocab_list = []\n\n        for i,y in enumerate(y_data):\n            for caption in y_data[f\"{i}\"]['caption']:\n                caption = \"<bos> \" + caption + \" <eos>\"\n                # if len(caption.split()) <= 15:\n                train_list.append([caption, y_data[f\"{i}\"]['id']])\n\n        random.shuffle(train_list)\n        training_list = train_list[int(len(train_list) * config.validation_split):]\n        validation_list = train_list[:int(len(train_list) * config.validation_split)]\n\n        for train in training_list:\n            vocab_list.append(train[0])\n        self.tokenizer = Tokenizer(num_words=config.num_decoder_tokens)\n        self.tokenizer.fit_on_texts(vocab_list)\n\n        TRAIN_FEATURE_DIR = os.path.join(config.train_feet, 'feat')\n        for filename in os.listdir(TRAIN_FEATURE_DIR):\n            f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename), allow_pickle=True)\n            self.x_data[filename[:-4]] = f\n\n        return training_list,self.x_data,validation_list\n\n    def load_dataset(self, training_list):\n\n        encoder_input_data = []\n        decoder_input_data = []\n        decoder_target_data = []\n        videoId = []\n        videoSeq = []\n\n        for idx, cap in enumerate(training_list): \n            caption = cap[0]\n            videoId.append(cap[1])\n            videoSeq.append(caption)\n        \n        train_sequences = self.tokenizer.texts_to_sequences(videoSeq)\n        train_sequences = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=config.max_length)\n        train_sequences = np.array(train_sequences)\n#         print(train_sequences)\n        file_size = len(train_sequences)\n        n = 0\n    \n        for idx in range(0, file_size):\n            n += 1\n            \n            encoder_input_data.append(self.x_data[videoId[idx]])\n            y = to_categorical(train_sequences[idx], config.num_decoder_tokens)\n            decoder_input_data.append(y[:-1])\n            decoder_target_data.append(y[1:])\n\n            if n == config.batch_size:\n                encoder_input_n = np.array(encoder_input_data)\n                decoder_input_n = np.array(decoder_input_data)\n                decoder_target_n = np.array(decoder_target_data)\n\n                # Convert data to PyTorch tensors\n                encoder_input = torch.tensor(encoder_input_n, dtype=torch.float32).to(device)\n                decoder_input = torch.tensor(decoder_input_n, dtype=torch.float32).to(device)\n                decoder_target = torch.tensor(decoder_target_n, dtype=torch.float32).to(device)\n\n                encoder_input_data = []\n                decoder_input_data = []\n                decoder_target_data = []\n                n = 0\n\n                yield ([encoder_input, decoder_input], decoder_target)\n\n\n    def train(self):\n\n        training_list, x_data,validation_list = self.preprocessing()\n\n        optimizer = optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=self.lr)\n        criterion = nn.CrossEntropyLoss()\n\n        training_losses = []\n        validation_losses = []\n        bleu_scores = []\n        best_val_loss = float('inf')\n        patience_counter = 0\n        run_epoch=0\n\n        for epoch in range(self.epochs):\n            self.encoder.train()\n            self.decoder.train()\n            train_loader = self.load_dataset(training_list)\n            total_train_loss = 0\n            train_steps = 0\n            run_epoch = run_epoch+1\n\n            for [encoder_input, decoder_input], decoder_target in train_loader:\n                decoder_target = torch.argmax(decoder_target, dim=-1)\n\n                encoder_state = self.encoder(encoder_input)\n                encoder_outputs, _ = self.encoder.encoder(encoder_input)  # Get encoder outputs for attention\n                decoder_output, attention_weights = self.decoder(decoder_input, encoder_state, encoder_outputs)\n\n      \n                loss = criterion(decoder_output.view(-1, config.num_decoder_tokens), decoder_target.view(-1))\n                total_train_loss += loss.item()\n\n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad()\n\n                train_steps += 1 \n                if train_steps % 500 == 0:\n                    print(f\"Epoch {epoch + 1}, Step {train_steps}: Training Loss = {total_train_loss / train_steps}\")\n\n            average_train_loss = total_train_loss / train_steps \n            \n            training_losses.append(average_train_loss)\n\n            self.encoder.eval()\n            self.decoder.eval()\n            total_val_loss = 0\n            val_steps = 0\n            total_sequences = 0\n            total_bleu = 0 \n            with torch.no_grad():  \n                valid_loader = self.load_dataset(validation_list)\n                for [encoder_input, decoder_input], decoder_target in valid_loader:\n                    decoder_target = torch.argmax(decoder_target, dim=-1)\n\n                    encoder_state = self.encoder(encoder_input)\n                    encoder_outputs, _ = self.encoder.encoder(encoder_input)  # Get encoder outputs for attention\n                    decoder_output, attention_weights = self.decoder(decoder_input, encoder_state, encoder_outputs)\n\n                    val_loss = criterion(decoder_output.view(-1, config.num_decoder_tokens), decoder_target.view(-1))\n                    total_val_loss += val_loss.item()\n                    predicted_sequences = torch.argmax(decoder_output, dim=-1)\n                    reference_sequences = decoder_target\n                    for i in range(len(predicted_sequences)):\n                        total_sequences += 1\n                        predicted_caption = [self.tokenizer.index_word.get(idx.item(), '<unk>') for idx in predicted_sequences[i] if idx > 2]\n                        reference_caption = [[self.tokenizer.index_word.get(idx.item(), '<unk>') for idx in reference_sequences[i] if idx > 2]]\n\n                        \n                        bleu_score = sentence_bleu(reference_caption, predicted_caption)\n                        total_bleu += bleu_score\n\n                    val_steps += 1  \n\n                average_bleu = total_bleu / total_sequences\n                bleu_scores.append(average_bleu)\n                average_val_loss = total_val_loss / val_steps \n                validation_losses.append(average_val_loss)# Calculate average validation loss\n                print(f'Epoch {epoch + 1}/{self.epochs}, Training Loss: {average_train_loss}, Validation Loss: {average_val_loss}, BLEU Score: {average_bleu}')\n                if average_val_loss < best_val_loss - self.min_delta:\n                    best_val_loss = average_val_loss\n                    patience_counter = 0  # Reset patience counter\n                    print(f\"Validation loss improved to {best_val_loss}. Saving model.\")\n                    torch.save(self.encoder.state_dict(), os.path.join(self.save_model_path, 'encoder_model_lstm.pth'))\n                    torch.save(self.decoder.state_dict(), os.path.join(self.save_model_path, 'decoder_model_lstm.pth'))\n                    with open(os.path.join(self.save_model_path, 'tokenizer_lstm_' + str(config.num_decoder_tokens)), 'wb') as file:\n                        joblib.dump(self.tokenizer, file)\n                else:\n                    patience_counter += 1\n                    print(f\"No improvement for {patience_counter} epochs.\")\n                    if patience_counter >= self.patience:\n                        print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n                        break\n\n        plt.figure(figsize=(10, 5))\n        plt.plot(range(1, run_epoch+1), training_losses, label='Training Loss')\n        plt.plot(range(1, run_epoch+1), validation_losses, label='Validation Loss')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.title('Training and Validation Loss per Epoch')\n        plt.legend()\n        plt.show()\n        plt.plot(range(1, run_epoch+1), bleu_scores, label='bleu_scores')\n        plt.show()    \nclass EncoderModel(nn.Module):\n    def __init__(self):\n        super(EncoderModel, self).__init__()\n        self.encoder = nn.LSTM(input_size=config.num_encoder_tokens, hidden_size=config.latent_dim, batch_first=True)\n        \n    def forward(self, encoder_inputs):\n        encoder_outputs, (state_h, state_c) = self.encoder(encoder_inputs)\n        return (state_h, state_c)\n        \nclass Attention(nn.Module):\n    def __init__(self, latent_dim):\n        super(Attention, self).__init__()\n        self.attn = nn.Linear(latent_dim * 2, latent_dim)\n        self.v = nn.Linear(latent_dim, 1, bias=False)\n\n    def forward(self, decoder_hidden, encoder_outputs):\n        # Expand decoder hidden state to match encoder outputs' time dimension\n        decoder_hidden_exp = decoder_hidden.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)\n        # Concatenate hidden state with encoder outputs\n        energy = torch.tanh(self.attn(torch.cat((decoder_hidden_exp, encoder_outputs), dim=2)))\n        # Compute attention scores\n        attention_scores = self.v(energy).squeeze(2)\n        attention_weights = torch.softmax(attention_scores, dim=1)\n        # Compute context vector as weighted sum of encoder outputs\n        context = torch.sum(attention_weights.unsqueeze(2) * encoder_outputs, dim=1)\n        return context, attention_weights\n\n\nclass DecoderModel(nn.Module):\n    def __init__(self):\n        super(DecoderModel, self).__init__()\n        self.decoder = nn.LSTM(input_size=config.num_decoder_tokens, hidden_size=config.latent_dim, batch_first=True)\n        self.attention = Attention(config.latent_dim)\n        self.concat = nn.Linear(config.latent_dim * 2, config.latent_dim)\n        self.decoder_dense = nn.Linear(config.latent_dim, config.num_decoder_tokens)\n        \n    def forward(self, decoder_inputs, encoder_states, encoder_outputs):\n        decoder_outputs, (hidden_state, cell_state) = self.decoder(decoder_inputs, encoder_states)\n        all_outputs = []\n        attention_weights_list = []\n\n        for t in range(decoder_outputs.size(1)):\n            # Select the current decoder hidden state\n            decoder_hidden = hidden_state[-1]  # Take the last layer's hidden state if multi-layered\n            # Apply attention mechanism\n            context, attention_weights = self.attention(decoder_hidden, encoder_outputs)\n            attention_weights_list.append(attention_weights)\n\n            # Concatenate context vector with current decoder hidden state\n            combined = torch.cat((decoder_outputs[:, t, :], context), dim=1)\n            combined = torch.tanh(self.concat(combined))\n            # Predict next token\n            output = self.decoder_dense(combined)\n            all_outputs.append(output.unsqueeze(1))\n\n        # Concatenate all time steps\n        final_outputs = torch.cat(all_outputs, dim=1)\n        return final_outputs, attention_weights_list\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-26T00:40:58.795996Z","iopub.execute_input":"2024-11-26T00:40:58.796350Z","iopub.status.idle":"2024-11-26T00:40:58.829589Z","shell.execute_reply.started":"2024-11-26T00:40:58.796319Z","shell.execute_reply":"2024-11-26T00:40:58.828674Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"dataset = VideoDataset()\ndataset.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T00:40:59.241057Z","iopub.execute_input":"2024-11-26T00:40:59.241985Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Step 500: Training Loss = 3.4440316853523254\nEpoch 1, Step 1000: Training Loss = 3.0967313017845153\nEpoch 1, Step 1500: Training Loss = 2.9095302421251934\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 3-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 2-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n/opt/conda/lib/python3.10/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \nCorpus/Sentence contains 0 counts of 4-gram overlaps.\nBLEU scores might be undesirable; use SmoothingFunction().\n  warnings.warn(_msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50, Training Loss: 2.8458278884777446, Validation Loss: 2.383697696591987, BLEU Score: 0.5995071963722498\nValidation loss improved to 2.383697696591987. Saving model.\nEpoch 2, Step 500: Training Loss = 2.358223240852356\nEpoch 2, Step 1000: Training Loss = 2.300657787680626\nEpoch 2, Step 1500: Training Loss = 2.2557233063379925\nEpoch 2/50, Training Loss: 2.238456957739902, Validation Loss: 2.154239591223295, BLEU Score: 0.603957579938259\nValidation loss improved to 2.154239591223295. Saving model.\nEpoch 3, Step 500: Training Loss = 2.094635508775711\nEpoch 3, Step 1000: Training Loss = 2.061631569504738\nEpoch 3, Step 1500: Training Loss = 2.0335416527589163\nEpoch 3/50, Training Loss: 2.0223116723788266, Validation Loss: 2.0623339105824954, BLEU Score: 0.5943438098553814\nValidation loss improved to 2.0623339105824954. Saving model.\nEpoch 4, Step 500: Training Loss = 1.9359647831916809\nEpoch 4, Step 1000: Training Loss = 1.9096849132776261\nEpoch 4, Step 1500: Training Loss = 1.887842086315155\nEpoch 4/50, Training Loss: 1.8791647893845, Validation Loss: 2.0277559073244937, BLEU Score: 0.589409310678616\nValidation loss improved to 2.0277559073244937. Saving model.\nEpoch 5, Step 500: Training Loss = 1.8156581616401672\nEpoch 5, Step 1000: Training Loss = 1.7898999456167222\nEpoch 5, Step 1500: Training Loss = 1.772301651875178\nEpoch 5/50, Training Loss: 1.7654457956380238, Validation Loss: 1.9884586537470583, BLEU Score: 0.6004554955074093\nValidation loss improved to 1.9884586537470583. Saving model.\nEpoch 6, Step 500: Training Loss = 1.7118407361507415\nEpoch 6, Step 1000: Training Loss = 1.6902035381793976\nEpoch 6, Step 1500: Training Loss = 1.6750854331652323\nEpoch 6/50, Training Loss: 1.6682344422175015, Validation Loss: 1.9528300926333568, BLEU Score: 0.59691956030309\nValidation loss improved to 1.9528300926333568. Saving model.\nEpoch 7, Step 500: Training Loss = 1.6219008047580719\nEpoch 7, Step 1000: Training Loss = 1.6014708364009858\nEpoch 7, Step 1500: Training Loss = 1.5868535267512003\nEpoch 7/50, Training Loss: 1.5799761626761772, Validation Loss: 1.9584491940795399, BLEU Score: 0.5946619893069105\nNo improvement for 1 epochs.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import random\n# import numpy as np\n# import config\n# from tensorflow.keras.preprocessing.text import Tokenizer\n# from keras.preprocessing.sequence import pad_sequences\n# from keras.utils import to_categorical\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import joblib\n# import matplotlib.pyplot as plt\n# import nltk\n# from nltk.translate.bleu_score import sentence_bleu\n\n# config.training_label = \"/kaggle/input/data-k\"\n# config.max_length = 15\n# config.epochs = 50\n# config.batch_size = 64\n# config.num_decoder_tokens = 6000\n# # config.num_decoder_tokens = 1500\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# print(device)\n# class VideoDataset():\n#     def __init__(self):\n#         self.x_data = {}\n#         self.encoder = EncoderModel().to(device)  \n#         self.decoder = TransformerDecoder().to(device)   \n#         self.lr = config.learning_rate\n#         self.epochs = config.epochs\n#         self.save_model_path = config.save_model_path\n#         self.patience = 3  \n#         self.min_delta = 0.001\n#     def preprocessing(self):\n  \n#         TRAIN_LABEL_PATH = os.path.join(config.training_label, '/kaggle/input/data-k/data (2).json')\n#         with open(TRAIN_LABEL_PATH) as data_file:\n#             y_data = json.load(data_file)\n        \n#         train_list = []\n#         vocab_list = []\n\n#         for i,y in enumerate(y_data):\n#             for caption in y_data[f\"{i}\"]['caption']:\n#                 caption = \"<bos> \" + caption + \" <eos>\"\n#                 # if len(caption.split()) <= 15:\n#                 train_list.append([caption, y_data[f\"{i}\"]['id']])\n\n#         random.shuffle(train_list)\n#         training_list = train_list[int(len(train_list) * config.validation_split):]\n#         validation_list = train_list[:int(len(train_list) * config.validation_split)]\n\n#         for train in training_list:\n#             vocab_list.append(train[0])\n#         self.tokenizer = Tokenizer(num_words=config.num_decoder_tokens)\n#         self.tokenizer.fit_on_texts(vocab_list)\n\n#         TRAIN_FEATURE_DIR = os.path.join(config.train_feet, 'feat')\n#         for filename in os.listdir(TRAIN_FEATURE_DIR):\n#             f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename), allow_pickle=True)\n#             self.x_data[filename[:-4]] = f\n\n#         return training_list,self.x_data,validation_list\n\n#     def load_dataset(self, training_list):\n\n#         encoder_input_data = []\n#         decoder_input_data = []\n#         decoder_target_data = []\n#         videoId = []\n#         videoSeq = []\n\n#         for idx, cap in enumerate(training_list): \n#             caption = cap[0]\n#             videoId.append(cap[1])\n#             videoSeq.append(caption)\n        \n#         train_sequences = self.tokenizer.texts_to_sequences(videoSeq)\n#         train_sequences = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=config.max_length)\n#         train_sequences = np.array(train_sequences)\n# #         print(train_sequences)\n#         file_size = len(train_sequences)\n#         n = 0\n    \n#         for idx in range(0, file_size):\n#             n += 1\n            \n#             encoder_input_data.append(self.x_data[videoId[idx]])\n#             y = to_categorical(train_sequences[idx], config.num_decoder_tokens)\n#             decoder_input_data.append(y[:-1])\n#             decoder_target_data.append(y[1:])\n\n#             if n == config.batch_size:\n#                 encoder_input_n = np.array(encoder_input_data)\n#                 decoder_input_n = np.array(decoder_input_data)\n#                 decoder_target_n = np.array(decoder_target_data)\n\n#                 # Convert data to PyTorch tensors\n#                 encoder_input = torch.tensor(encoder_input_n, dtype=torch.float32).to(device)\n#                 decoder_input = torch.tensor(decoder_input_n, dtype=torch.float32).to(device)\n#                 decoder_target = torch.tensor(decoder_target_n, dtype=torch.float32).to(device)\n\n#                 encoder_input_data = []\n#                 decoder_input_data = []\n#                 decoder_target_data = []\n#                 n = 0\n\n#                 yield ([encoder_input, decoder_input], decoder_target)\n\n\n#     def train(self):\n\n#         training_list, x_data,validation_list = self.preprocessing()\n\n#         optimizer = optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=self.lr)\n#         criterion = nn.CrossEntropyLoss()\n\n#         training_losses = []\n#         validation_losses = []\n#         bleu_scores = []\n#         best_val_loss = float('inf')\n#         patience_counter = 0\n#         run_epoch=0\n\n#         for epoch in range(self.epochs):\n#             self.encoder.train()\n#             self.decoder.train()\n#             train_loader = self.load_dataset(training_list)\n#             total_train_loss = 0\n#             train_steps = 0\n#             run_epoch = run_epoch+1\n\n#             for [encoder_input, decoder_input], decoder_target in train_loader:\n#                 decoder_target = torch.argmax(decoder_target, dim=-1)\n\n#                 encoder_state = self.encoder(encoder_input)\n#                 decoder_output = self.decoder(decoder_input, encoder_state)\n\n      \n#                 loss = criterion(decoder_output.view(-1, config.num_decoder_tokens), decoder_target.view(-1))\n#                 total_train_loss += loss.item()\n\n#                 loss.backward()\n#                 optimizer.step()\n#                 optimizer.zero_grad()\n\n#                 train_steps += 1 \n#                 if train_steps % 500 == 0:\n#                     print(f\"Epoch {epoch + 1}, Step {train_steps}: Training Loss = {total_train_loss / train_steps}\")\n\n#             average_train_loss = total_train_loss / train_steps \n            \n#             training_losses.append(average_train_loss)\n\n#             self.encoder.eval()\n#             self.decoder.eval()\n#             total_val_loss = 0\n#             val_steps = 0\n#             total_sequences = 0\n#             total_bleu = 0 \n#             with torch.no_grad():  \n#                 valid_loader = self.load_dataset(validation_list)\n#                 for [encoder_input, decoder_input], decoder_target in valid_loader:\n#                     decoder_target = torch.argmax(decoder_target, dim=-1)\n\n#                     encoder_state = self.encoder(encoder_input)\n#                     decoder_output = self.decoder(decoder_input, encoder_state)\n#                     val_loss = criterion(decoder_output.view(-1, config.num_decoder_tokens), decoder_target.view(-1))\n#                     total_val_loss += val_loss.item()\n#                     predicted_sequences = torch.argmax(decoder_output, dim=-1)\n#                     reference_sequences = decoder_target\n#                     for i in range(len(predicted_sequences)):\n#                         total_sequences += 1\n#                         predicted_caption = [self.tokenizer.index_word.get(idx.item(), '<unk>') for idx in predicted_sequences[i] if idx > 2]\n#                         reference_caption = [[self.tokenizer.index_word.get(idx.item(), '<unk>') for idx in reference_sequences[i] if idx > 2]]\n\n                        \n#                         bleu_score = sentence_bleu(reference_caption, predicted_caption)\n#                         total_bleu += bleu_score\n\n#                     val_steps += 1  \n\n#                 average_bleu = total_bleu / total_sequences\n#                 bleu_scores.append(average_bleu)\n#                 average_val_loss = total_val_loss / val_steps \n#                 validation_losses.append(average_val_loss)# Calculate average validation loss\n#                 print(f'Epoch {epoch + 1}/{self.epochs}, Training Loss: {average_train_loss}, Validation Loss: {average_val_loss}, BLEU Score: {average_bleu}')\n#                 if average_val_loss < best_val_loss - self.min_delta:\n#                     best_val_loss = average_val_loss\n#                     patience_counter = 0  # Reset patience counter\n#                     print(f\"Validation loss improved to {best_val_loss}. Saving model.\")\n#                     torch.save(self.encoder.state_dict(), os.path.join(self.save_model_path, 'encoder_model_lstm.pth'))\n#                     torch.save(self.decoder.state_dict(), os.path.join(self.save_model_path, 'decoder_model_lstm.pth'))\n#                     with open(os.path.join(self.save_model_path, 'tokenizer_lstm_' + str(config.num_decoder_tokens)), 'wb') as file:\n#                         joblib.dump(self.tokenizer, file)\n#                 else:\n#                     patience_counter += 1\n#                     print(f\"No improvement for {patience_counter} epochs.\")\n#                     if patience_counter >= self.patience:\n#                         print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n#                         break\n\n#         plt.figure(figsize=(10, 5))\n#         plt.plot(range(1, run_epoch+1), training_losses, label='Training Loss')\n#         plt.plot(range(1, run_epoch+1), validation_losses, label='Validation Loss')\n#         plt.xlabel('Epochs')\n#         plt.ylabel('Loss')\n#         plt.title('Training and Validation Loss per Epoch')\n#         plt.legend()\n#         plt.show()\n#         plt.plot(range(1, run_epoch+1), bleu_scores, label='bleu_scores')\n#         plt.show()    \n\n# class EncoderModel(nn.Module):\n#     def __init__(self):\n#         super(EncoderModel, self).__init__()\n#         self.encoder = nn.LSTM(input_size=config.num_encoder_tokens, hidden_size=config.latent_dim, batch_first=True)\n        \n#     def forward(self, encoder_inputs):\n#         encoder_outputs, (state_h, state_c) = self.encoder(encoder_inputs)\n#         return (state_h, state_c)\n\n\n# class TransformerDecoder(nn.Module):\n#     def __init__(self):\n#         super(TransformerDecoder, self).__init__()\n#         self.embedding = nn.Embedding(config.num_decoder_tokens, config.latent_dim)\n#         self.positional_encoding = nn.Parameter(torch.zeros(1, config.max_length, config.latent_dim))\n#         self.transformer_decoder = nn.Transformer(d_model=config.latent_dim, nhead=8, num_encoder_layers=6, num_decoder_layers=6)\n#         self.fc_out = nn.Linear(config.latent_dim, config.num_decoder_tokens)\n\n#     def forward(self, decoder_inputs, encoder_states):\n#         embedded_inputs = self.embedding(decoder_inputs) + self.positional_encoding[:, :decoder_inputs.size(1), :]\n#         transformer_output = self.transformer_decoder(embedded_inputs, encoder_states[0].unsqueeze(0))\n#         output = self.fc_out(transformer_output)\n#         return output\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T00:30:10.287100Z","iopub.execute_input":"2024-11-26T00:30:10.287402Z","iopub.status.idle":"2024-11-26T00:30:10.307082Z","shell.execute_reply.started":"2024-11-26T00:30:10.287374Z","shell.execute_reply":"2024-11-26T00:30:10.306174Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"","metadata":{"execution":{"iopub.status.busy":"2024-11-11T20:36:46.330552Z","iopub.execute_input":"2024-11-11T20:36:46.330984Z","iopub.status.idle":"2024-11-11T20:36:46.371592Z","shell.execute_reply.started":"2024-11-11T20:36:46.330943Z","shell.execute_reply":"2024-11-11T20:36:46.370633Z"}}},{"cell_type":"code","source":"# import os\n# import json\n# import random\n# import numpy as np\n# import config\n# from tensorflow.keras.preprocessing.text import Tokenizer\n# from keras.preprocessing.sequence import pad_sequences\n# from keras.utils import to_categorical\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import joblib\n# import matplotlib.pyplot as plt\n# import nltk\n# from nltk.translate.bleu_score import sentence_bleu\n\n# config.training_label = \"/kaggle/input/data-k\"\n# config.max_length = 15\n# config.epochs = 50\n# config.batch_size = 64\n# config.num_decoder_tokens = 6000\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# print(device)\n\n# class VideoDataset():\n#     def __init__(self):\n#         self.x_data = {}\n#         self.encoder = EncoderModel().to(device)\n#         self.decoder = TransformerDecoder().to(device)  \n#         self.lr = config.learning_rate\n#         self.epochs = config.epochs\n#         self.save_model_path = config.save_model_path\n#         self.patience = 3  \n#         self.min_delta = 0.001\n\n#     def preprocessing(self):\n#         TRAIN_LABEL_PATH = os.path.join(config.training_label, '/kaggle/input/data-k/data (2).json')\n#         with open(TRAIN_LABEL_PATH) as data_file:\n#             y_data = json.load(data_file)\n        \n#         train_list = []\n#         vocab_list = []\n\n#         for i, y in enumerate(y_data):\n#             for caption in y_data[f\"{i}\"]['caption']:\n#                 caption = \"<bos> \" + caption + \" <eos>\"\n#                 train_list.append([caption, y_data[f\"{i}\"]['id']])\n\n#         random.shuffle(train_list)\n#         training_list = train_list[int(len(train_list) * config.validation_split):]\n#         validation_list = train_list[:int(len(train_list) * config.validation_split)]\n\n#         for train in training_list:\n#             vocab_list.append(train[0])\n#         self.tokenizer = Tokenizer(num_words=config.num_decoder_tokens)\n#         self.tokenizer.fit_on_texts(vocab_list)\n\n#         TRAIN_FEATURE_DIR = os.path.join(config.train_feet, 'feat')\n#         for filename in os.listdir(TRAIN_FEATURE_DIR):\n#             f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename), allow_pickle=True)\n#             self.x_data[filename[:-4]] = f\n\n#         return training_list, self.x_data, validation_list\n\n#     def load_dataset(self, training_list):\n#         encoder_input_data = []\n#         decoder_input_data = []\n#         decoder_target_data = []\n#         videoId = []\n#         videoSeq = []\n\n#         for idx, cap in enumerate(training_list):\n#             caption = cap[0]\n#             videoId.append(cap[1])\n#             videoSeq.append(caption)\n        \n#         train_sequences = self.tokenizer.texts_to_sequences(videoSeq)\n#         train_sequences = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=config.max_length)\n#         train_sequences = np.array(train_sequences)\n\n#         file_size = len(train_sequences)\n#         n = 0\n    \n#         for idx in range(0, file_size):\n#             n += 1\n            \n#             encoder_input_data.append(self.x_data[videoId[idx]])\n#             y = to_categorical(train_sequences[idx], config.num_decoder_tokens)\n#             decoder_input_data.append(y[:-1])\n#             decoder_target_data.append(y[1:])\n\n#             if n == config.batch_size:\n#                 encoder_input_n = np.array(encoder_input_data)\n#                 decoder_input_n = np.array(decoder_input_data)\n#                 decoder_target_n = np.array(decoder_target_data)\n\n#                 encoder_input = torch.tensor(encoder_input_n, dtype=torch.float32).to(device)\n#                 decoder_input = torch.tensor(decoder_input_n, dtype=torch.float32).to(device)\n#                 decoder_target = torch.tensor(decoder_target_n, dtype=torch.float32).to(device)\n\n#                 encoder_input_data = []\n#                 decoder_input_data = []\n#                 decoder_target_data = []\n#                 n = 0\n\n#                 yield ([encoder_input, decoder_input], decoder_target)\n\n#     def train(self):\n#         training_list, x_data, validation_list = self.preprocessing()\n\n#         optimizer = optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=self.lr)\n#         criterion = nn.CrossEntropyLoss()\n\n#         training_losses = []\n#         validation_losses = []\n#         bleu_scores = []\n#         best_val_loss = float('inf')\n#         patience_counter = 0\n#         run_epoch = 0\n\n#         for epoch in range(self.epochs):\n#             self.encoder.train()\n#             self.decoder.train()\n#             train_loader = self.load_dataset(training_list)\n#             total_train_loss = 0\n#             train_steps = 0\n#             run_epoch += 1\n\n#             for [encoder_input, decoder_input], decoder_target in train_loader:\n#                 decoder_target = torch.argmax(decoder_target, dim=-1)\n                \n#                 encoder_state = self.encoder(encoder_input)\n#                 decoder_output = self.decoder(decoder_input, encoder_state)\n#                 print(decoder_output.shape)\n#                 print(decoder_target.shape)\n#                 loss = criterion(decoder_output.view(-1, config.num_decoder_tokens), decoder_target.view(-1))\n#                 total_train_loss += loss.item()\n\n#                 loss.backward()\n#                 optimizer.step()\n#                 optimizer.zero_grad()\n\n#                 train_steps += 1 \n#                 if train_steps % 500 == 0:\n#                     print(f\"Epoch {epoch + 1}, Step {train_steps}: Training Loss = {total_train_loss / train_steps}\")\n\n#             average_train_loss = total_train_loss / train_steps\n#             training_losses.append(average_train_loss)\n\n#             # Validation loop...\n#             # (similar to training loop but without `loss.backward()`)\n\n#         # Plotting loss and BLEU scores\n#         plt.figure(figsize=(10, 5))\n#         plt.plot(range(1, run_epoch+1), training_losses, label='Training Loss')\n#         plt.xlabel('Epochs')\n#         plt.ylabel('Loss')\n#         plt.title('Training and Validation Loss per Epoch')\n#         plt.legend()\n#         plt.show()\n\n# class EncoderModel(nn.Module):\n#     def __init__(self):\n#         super(EncoderModel, self).__init__()\n#         self.encoder = nn.LSTM(input_size=config.num_encoder_tokens, hidden_size=config.latent_dim, batch_first=True)\n        \n#     def forward(self, encoder_inputs):\n#         encoder_outputs, (state_h, state_c) = self.encoder(encoder_inputs)\n#         return (state_h, state_c)\n\n# class TransformerDecoder(nn.Module):\n#     def __init__(self):\n#         super(TransformerDecoder, self).__init__()\n#         self.embedding = nn.Embedding(config.num_decoder_tokens, 512)  # Embedding layer with latent_dim\n#         self.positional_encoding = nn.Parameter(torch.zeros(64, config.max_length-1, config.latent_dim))  # Adjusting to latent_dim size\n#         self.transformer_decoder = nn.Transformer(d_model=config.latent_dim, nhead=8, num_encoder_layers=6, num_decoder_layers=6)\n#         self.fc_out = nn.Linear(config.latent_dim, config.num_decoder_tokens)\n\n#     def forward(self, decoder_inputs, encoder_states):\n#         # Convert decoder inputs to Long type for embedding lookup\n#         decoder_inputs = torch.argmax(decoder_inputs, dim=-1)\n#         embedded_inputs = self.embedding(decoder_inputs.long())\n#         print(embedded_inputs.shape)\n#         # Adjust positional encoding to match the input sequence length\n#         position_encoding = self.positional_encoding\n#         print(position_encoding.shape)\n#         # Adding embeddings and positional encodings\n#         embedded_inputs = embedded_inputs + position_encoding\n#         print(embedded_inputs.shape)\n#         # Permute encoder_states to (seq_len, batch, latent_dim) for nn.Transformer\n#         encoder_states = encoder_states[0]\n#         print(encoder_states.shape)\n#         # Forward pass through Transformerecoder\n#         transformer_output = self.transformer_decoder(embedded_inputs.permute(1, 0, 2), encoder_states)\n        \n#         # Permute transforme dr_output back to (batch, seq_len, latent_dim) and apply final linear layer\n#         output = self.fc_out(transformer_output.permute(1, 0, 2))\n#         return output\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T00:30:10.309003Z","iopub.execute_input":"2024-11-26T00:30:10.309319Z","iopub.status.idle":"2024-11-26T00:30:10.337018Z","shell.execute_reply.started":"2024-11-26T00:30:10.309292Z","shell.execute_reply":"2024-11-26T00:30:10.336096Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# dataset = VideoDataset()\n# # dataset.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-26T00:31:26.217656Z","iopub.execute_input":"2024-11-26T00:31:26.218032Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1, Step 500: Training Loss = 3.4462828130722047\nEpoch 1, Step 1000: Training Loss = 3.1145551958084106\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# import os\n# import json\n# import random\n# import numpy as np\n# import config\n# from tensorflow.keras.preprocessing.text import Tokenizer\n# from keras.preprocessing.sequence import pad_sequences\n# from keras.utils import to_categorical\n# import torch\n# import torch.nn as nn\n# import torch.optim as optim\n# import joblib\n# import matplotlib.pyplot as plt\n# import nltk\n# from nltk.translate.bleu_score import sentence_bleu\n\n\n\n# config.training_label = \"/kaggle/input/data-k\"\n# config.max_length = 15\n# config.epochs = 50\n# config.batch_size = 64\n# config.num_decoder_tokens = 6000\n# # config.num_decoder_tokens = 1500\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# print(device)\n# class VideoDataset():\n#     def __init__(self):\n#         self.x_data = {}\n#         self.encoder = EncoderModel().to(device)  \n#         self.decoder = DecoderModel().to(device)  \n#         self.lr = config.learning_rate\n#         self.epochs = config.epochs\n#         self.save_model_path = config.save_model_path\n#         self.patience = 3  \n#         self.min_delta = 0.001\n#     def preprocessing(self):\n#         TRAIN_LABEL_PATH = os.path.join(config.training_label, '/kaggle/input/data-k/data (2).json')\n#         with open(TRAIN_LABEL_PATH) as data_file:\n#             y_data = json.load(data_file)\n        \n#         train_list = []\n#         vocab_list = []\n\n#         for i,y in enumerate(y_data):\n#             for caption in y_data[f\"{i}\"]['caption']:\n#                 caption = \"<bos> \" + caption + \" <eos>\"\n#                 # if len(caption.split()):\n#                 train_list.append([caption, y_data[f\"{i}\"]['id']])\n\n#         random.shuffle(train_list)\n#         training_list = train_list[int(len(train_list) * config.validation_split):]\n#         validation_list = train_list[:int(len(train_list) * config.validation_split)]\n\n#         for train in training_list:\n#             vocab_list.append(train[0])\n#         self.tokenizer = Tokenizer(num_words=config.num_decoder_tokens)\n#         self.tokenizer.fit_on_texts(vocab_list)\n\n#         TRAIN_FEATURE_DIR = os.path.join(config.train_feet, 'feat')\n#         for filename in os.listdir(TRAIN_FEATURE_DIR):\n#             f = np.load(os.path.join(TRAIN_FEATURE_DIR, filename), allow_pickle=True)\n#             self.x_data[filename[:-4]] = f\n\n#         return training_list,self.x_data,validation_list\n\n#     def load_dataset(self, training_list):\n\n#         encoder_input_data = []\n#         decoder_input_data = []\n#         decoder_target_data = []\n#         videoId = []\n#         videoSeq = []\n    \n#         for idx, cap in enumerate(training_list): \n#             caption = cap[0]\n#             videoId.append(cap[1])\n#             videoSeq.append(caption)\n        \n#         train_sequences = self.tokenizer.texts_to_sequences(videoSeq)\n#         train_sequences = pad_sequences(train_sequences, padding='post', truncating='post', maxlen=config.max_length)\n#         train_sequences = np.array(train_sequences)\n        \n#         file_size = len(train_sequences)\n#         n = 0\n    \n#         for idx in range(0, file_size):\n#             n += 1\n            \n#             encoder_input_data.append(self.x_data[videoId[idx]])\n#             y = to_categorical(train_sequences[idx], config.num_decoder_tokens)\n#             decoder_input_data.append(y[:-1])\n#             decoder_target_data.append(y[1:])\n\n#             if n == config.batch_size:\n#                 encoder_input_n = np.array(encoder_input_data)\n#                 decoder_input_n = np.array(decoder_input_data)\n#                 decoder_target_n = np.array(decoder_target_data)\n\n#                 # Convert data to PyTorch tensors\n#                 encoder_input = torch.tensor(encoder_input_n, dtype=torch.float32).to(device)\n#                 decoder_input = torch.tensor(decoder_input_n, dtype=torch.float32).to(device)\n#                 decoder_target = torch.tensor(decoder_target_n, dtype=torch.float32).to(device)\n\n#                 encoder_input_data = []\n#                 decoder_input_data = []\n#                 decoder_target_data = []\n#                 n = 0\n\n#                 yield ([encoder_input, decoder_input], decoder_target)\n\n\n#     def train(self):\n\n#         training_list, x_data,validation_list = self.preprocessing()\n\n#         optimizer = optim.Adam(list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=self.lr)\n#         criterion = nn.CrossEntropyLoss()\n\n#         training_losses = []\n#         validation_losses = []\n#         bleu_scores = []\n#         best_val_loss = float('inf')\n#         patience_counter = 0\n#         run_epoch=0\n\n#         for epoch in range(self.epochs):\n#             # Training loop\n#             self.encoder.train()\n#             self.decoder.train()\n#             train_loader = self.load_dataset(training_list)\n#             total_train_loss = 0\n#             train_steps = 0\n#             run_epoch = run_epoch+1\n\n#             for [encoder_input, decoder_input], decoder_target in train_loader:\n\n#                 decoder_target = torch.argmax(decoder_target, dim=-1) \n\n#                 encoder_state = self.encoder(encoder_input)\n#                 decoder_output = self.decoder(decoder_input, encoder_state)\n\n#                 loss = criterion(decoder_output.view(-1, config.num_decoder_tokens), decoder_target.view(-1))\n#                 total_train_loss += loss.item()\n#                 loss.backward()\n#                 optimizer.step()\n#                 optimizer.zero_grad()\n\n#                 train_steps += 1 \n#                 if train_steps % 500 == 0:\n#                     print(f\"Epoch {epoch + 1}, Step {train_steps}: Training Loss = {total_train_loss / train_steps}\")\n\n#             average_train_loss = total_train_loss / train_steps \n#             training_losses.append(average_train_loss)\n\n#             self.encoder.eval()\n#             self.decoder.eval()\n#             total_val_loss = 0\n#             val_steps = 0\n#             total_sequences = 0\n#             total_bleu = 0 \n#             with torch.no_grad(): \n#                 valid_loader = self.load_dataset(validation_list)\n#                 for [encoder_input, decoder_input], decoder_target in valid_loader:\n#                     decoder_target = torch.argmax(decoder_target, dim=-1)\n#                     encoder_state = self.encoder(encoder_input)\n#                     decoder_output = self.decoder(decoder_input, encoder_state)\n#                     val_loss = criterion(decoder_output.view(-1, config.num_decoder_tokens), decoder_target.view(-1))\n#                     total_val_loss += val_loss.item()\n#                     predicted_sequences = torch.argmax(decoder_output, dim=-1)\n#                     reference_sequences = decoder_target\n#                     for i in range(len(predicted_sequences)):\n#                         total_sequences += 1\n#                         predicted_caption = [self.tokenizer.index_word.get(idx.item(), '<unk>') for idx in predicted_sequences[i] if idx > 2]\n#                         reference_caption = [[self.tokenizer.index_word.get(idx.item(), '<unk>') for idx in reference_sequences[i] if idx > 2]]\n\n                \n#                         bleu_score = sentence_bleu(reference_caption, predicted_caption)\n#                         total_bleu += bleu_score\n\n#                     val_steps += 1  \n                    \n#                 average_bleu = total_bleu / total_sequences\n#                 bleu_scores.append(average_bleu)\n#                 average_val_loss = total_val_loss / val_steps \n#                 validation_losses.append(average_val_loss)# Calculate average validation loss\n#                 print(f'Epoch {epoch + 1}/{self.epochs}, Training Loss: {average_train_loss}, Validation Loss: {average_val_loss}, BLEU Score: {average_bleu}')\n#                 if average_val_loss < best_val_loss - self.min_delta:\n#                     best_val_loss = average_val_loss\n#                     patience_counter = 0  # Reset patience counter\n#                     print(f\"Validation loss improved to {best_val_loss}. Saving model.\")\n#                     torch.save(self.encoder.state_dict(), os.path.join(self.save_model_path, 'encoder_model_gru.pth'))\n#                     torch.save(self.decoder.state_dict(), os.path.join(self.save_model_path, 'decoder_model_gru.pth'))\n#                     with open(os.path.join(self.save_model_path, 'tokenizer_gru_' + str(config.num_decoder_tokens)), 'wb') as file:\n#                         joblib.dump(self.tokenizer, file)\n#                 else:\n#                     patience_counter += 1\n#                     print(f\"No improvement for {patience_counter} epochs.\")\n#                     if patience_counter >= self.patience:\n#                         print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n#                         break\n                        \n#         plt.figure(figsize=(10, 5))\n#         plt.plot(range(1, run_epoch+1), training_losses, label='Training Loss')\n#         plt.plot(range(1, run_epoch+1), validation_losses, label='Validation Loss')\n#         plt.xlabel('Epochs')\n#         plt.ylabel('Loss')\n#         plt.title('Training and Validation Loss per Epoch')\n#         plt.legend()\n#         plt.show()\n#         plt.plot(range(1, run_epoch+1), bleu_scores, label='bleu_scores')\n#         plt.show()    \n\n# class EncoderModel(nn.Module):\n#     def __init__(self):\n#         super(EncoderModel, self).__init__()\n\n#         self.encoder = nn.GRU(input_size=config.num_encoder_tokens, hidden_size=config.latent_dim, batch_first=True)\n        \n#     def forward(self, encoder_inputs):\n        \n#         encoder_outputs, state_h = self.encoder(encoder_inputs)\n#         return state_h  \n\n\n\n# class Attention(nn.Module):\n#     def __init__(self, latent_dim):\n#         super(Attention, self).__init__()\n#         self.attention = nn.Linear(latent_dim * 2, latent_dim)\n#         self.v = nn.Parameter(torch.rand(latent_dim))\n\n#     def forward(self, decoder_hidden, encoder_outputs):\n#         \"\"\"\n#         decoder_hidden: (batch_size, latent_dim)\n#         encoder_outputs: (batch_size, seq_len, latent_dim)\n#         \"\"\"\n#         seq_len = encoder_outputs.size(1)\n#         decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)  # (batch_size, seq_len, latent_dim)\n#         combined = torch.cat((decoder_hidden, encoder_outputs), dim=2)      # (batch_size, seq_len, latent_dim * 2)\n#         energy = torch.tanh(self.attention(combined))                       # (batch_size, seq_len, latent_dim)\n#         energy = energy @ self.v                                           # (batch_size, seq_len)\n#         attention_weights = F.softmax(energy, dim=1)                       # (batch_size, seq_len)\n#         context_vector = attention_weights.unsqueeze(1) @ encoder_outputs  # (batch_size, 1, latent_dim)\n#         return context_vector.squeeze(1), attention_weights                # (batch_size, latent_dim), (batch_size, seq_len)\n\n# class DecoderModel(nn.Module):\n#     def __init__(self):\n#         super(DecoderModel, self).__init__()\n\n#         self.decoder = nn.GRU(input_size=config.num_decoder_tokens + config.latent_dim, \n#                               hidden_size=config.latent_dim, \n#                               batch_first=True)\n#         self.attention = Attention(config.latent_dim)\n#         self.decoder_dense = nn.Linear(config.latent_dim, config.num_decoder_tokens)\n\n#     def forward(self, decoder_inputs, encoder_outputs, decoder_hidden):\n#         \"\"\"\n#         decoder_inputs: (batch_size, seq_len, num_decoder_tokens)\n#         encoder_outputs: (batch_size, encoder_seq_len, latent_dim)\n#         decoder_hidden: (1, batch_size, latent_dim)\n#         \"\"\"\n#         seq_len = decoder_inputs.size(1)\n#         all_outputs = []\n\n#         for t in range(seq_len):\n#             decoder_input_t = decoder_inputs[:, t, :].unsqueeze(1)  # (batch_size, 1, num_decoder_tokens)\n#             context_vector, attention_weights = self.attention(decoder_hidden[-1], encoder_outputs)  # (batch_size, latent_dim)\n#             context_vector = context_vector.unsqueeze(1)           # (batch_size, 1, latent_dim)\n#             decoder_input_combined = torch.cat((decoder_input_t, context_vector), dim=2)  # (batch_size, 1, input_size + latent_dim)\n#             decoder_output, decoder_hidden = self.decoder(decoder_input_combined, decoder_hidden)  # (batch_size, 1, latent_dim), (1, batch_size, latent_dim)\n#             all_outputs.append(decoder_output)\n        \n#         decoder_outputs = torch.cat(all_outputs, dim=1)  # (batch_size, seq_len, latent_dim)\n#         final_outputs = self.decoder_dense(decoder_outputs)  # (batch_size, seq_len, num_decoder_tokens)\n#         return final_outputs, attention_weights\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-11-26T00:31:05.838672Z","iopub.status.idle":"2024-11-26T00:31:05.839058Z","shell.execute_reply.started":"2024-11-26T00:31:05.838880Z","shell.execute_reply":"2024-11-26T00:31:05.838900Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  ","metadata":{"execution":{"iopub.status.busy":"2024-11-26T00:31:05.840040Z","iopub.status.idle":"2024-11-26T00:31:05.840418Z","shell.execute_reply.started":"2024-11-26T00:31:05.840235Z","shell.execute_reply":"2024-11-26T00:31:05.840262Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}